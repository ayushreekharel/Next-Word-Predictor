{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "faqs=\"\"\"Breakfast is often hailed as the most important meal of the day, providing the fuel our bodies need after a night of rest and setting the tone for our energy levels, focus, and mood throughout the morning. A nutritious breakfast can have numerous health benefits, impacting physical well-being, cognitive performance, and overall daily productivity. Typically, breakfast replenishes glucose levels, offering a crucial energy boost essential for engaging in morning activities and staying alert. Studies reveal that a balanced breakfast, one containing carbohydrates, proteins, and healthy fats, can improve cognitive functions like memory and concentration. This is particularly beneficial for students and working professionals, as it enhances mental performance, allowing them to retain information more effectively and stay focused. Skipping breakfast, however, may lead to sluggishness and irritability, as blood sugar levels remain low, impairing cognitive ability and diminishing one’s ability to handle tasks efficiently. For those managing weight, breakfast plays an important role in controlling appetite throughout the day. When people skip breakfast, they often overcompensate by eating larger portions at subsequent meals or snacking on high-calorie foods to curb hunger, which can contribute to unhealthy weight gain. Including protein-rich foods like eggs, Greek yogurt, or nuts, alongside whole grains and fruits, not only curbs hunger but also stabilizes energy, ensuring a slower release of sugar into the bloodstream. Moreover, people who regularly eat breakfast tend to make healthier food choices throughout the day, as breakfast can help regulate cravings and reduce impulsive eating. Another significant benefit of breakfast is its positive impact on metabolic rate. When we eat in the morning, our metabolism kicks in, aiding in better digestion and nutrient absorption. A high-fiber breakfast, such as oatmeal or whole-grain toast, supports digestive health by promoting regular bowel movements and preventing constipation, which is vital for overall gut health. Furthermore, breakfast foods can vary greatly to accommodate dietary restrictions, cultural differences, and personal preferences. For example, a traditional Western breakfast might include cereals, eggs, or toast, while in Asian cultures, breakfast could consist of rice dishes, soups, or dumplings. Although breakfast habits vary, the core goal remains the same: to provide the body with essential nutrients to begin the day. Additionally, breakfast can be an opportunity to include important vitamins and minerals, as people are more likely to consume fruits, vegetables, and dairy in the morning. This can contribute to meeting the recommended daily intake of calcium, iron, and vitamins B and C, which are essential for maintaining bone health, muscle function, and immune response. There is also a social aspect to breakfast; sharing a morning meal with family or friends can foster a sense of connection and well-being. This routine allows individuals to start the day on a positive note, often encouraging mindful eating habits and making breakfast a moment to pause before the demands of the day take over. For many people, however, busy lifestyles make it challenging to fit in a proper breakfast. Despite time constraints, breakfast can be made more accessible with quick yet nutritious options such as smoothies, whole-grain muffins, or breakfast bars. Ultimately, breakfast is a powerful tool for enhancing health, managing weight, and promoting mental clarity and productivity. Whether enjoyed leisurely or on the go, a well-balanced breakfast contributes substantially to one's physical and mental health, underscoring why it remains such a fundamental part of a healthy lifestyle.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts([faqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 1,\n",
       " 'breakfast': 2,\n",
       " 'the': 3,\n",
       " 'a': 4,\n",
       " 'to': 5,\n",
       " 'of': 6,\n",
       " 'for': 7,\n",
       " 'can': 8,\n",
       " 'in': 9,\n",
       " 'or': 10,\n",
       " 'as': 11,\n",
       " 'is': 12,\n",
       " 'day': 13,\n",
       " 'health': 14,\n",
       " 'morning': 15,\n",
       " 'people': 16,\n",
       " 'on': 17,\n",
       " 'often': 18,\n",
       " 'important': 19,\n",
       " 'our': 20,\n",
       " 'energy': 21,\n",
       " 'levels': 22,\n",
       " 'throughout': 23,\n",
       " 'well': 24,\n",
       " 'cognitive': 25,\n",
       " 'essential': 26,\n",
       " 'this': 27,\n",
       " 'it': 28,\n",
       " 'mental': 29,\n",
       " 'more': 30,\n",
       " 'weight': 31,\n",
       " 'eating': 32,\n",
       " 'foods': 33,\n",
       " 'which': 34,\n",
       " 'whole': 35,\n",
       " 'such': 36,\n",
       " 'with': 37,\n",
       " 'meal': 38,\n",
       " 'nutritious': 39,\n",
       " 'physical': 40,\n",
       " 'being': 41,\n",
       " 'performance': 42,\n",
       " 'overall': 43,\n",
       " 'daily': 44,\n",
       " 'productivity': 45,\n",
       " 'balanced': 46,\n",
       " 'healthy': 47,\n",
       " 'like': 48,\n",
       " 'however': 49,\n",
       " 'sugar': 50,\n",
       " 'ability': 51,\n",
       " 'managing': 52,\n",
       " 'an': 53,\n",
       " 'when': 54,\n",
       " 'by': 55,\n",
       " 'high': 56,\n",
       " 'hunger': 57,\n",
       " 'contribute': 58,\n",
       " 'eggs': 59,\n",
       " 'fruits': 60,\n",
       " 'also': 61,\n",
       " 'eat': 62,\n",
       " 'make': 63,\n",
       " 'positive': 64,\n",
       " 'grain': 65,\n",
       " 'toast': 66,\n",
       " 'promoting': 67,\n",
       " 'vary': 68,\n",
       " 'include': 69,\n",
       " 'habits': 70,\n",
       " 'remains': 71,\n",
       " 'be': 72,\n",
       " 'vitamins': 73,\n",
       " 'are': 74,\n",
       " 'hailed': 75,\n",
       " 'most': 76,\n",
       " 'providing': 77,\n",
       " 'fuel': 78,\n",
       " 'bodies': 79,\n",
       " 'need': 80,\n",
       " 'after': 81,\n",
       " 'night': 82,\n",
       " 'rest': 83,\n",
       " 'setting': 84,\n",
       " 'tone': 85,\n",
       " 'focus': 86,\n",
       " 'mood': 87,\n",
       " 'have': 88,\n",
       " 'numerous': 89,\n",
       " 'benefits': 90,\n",
       " 'impacting': 91,\n",
       " 'typically': 92,\n",
       " 'replenishes': 93,\n",
       " 'glucose': 94,\n",
       " 'offering': 95,\n",
       " 'crucial': 96,\n",
       " 'boost': 97,\n",
       " 'engaging': 98,\n",
       " 'activities': 99,\n",
       " 'staying': 100,\n",
       " 'alert': 101,\n",
       " 'studies': 102,\n",
       " 'reveal': 103,\n",
       " 'that': 104,\n",
       " 'one': 105,\n",
       " 'containing': 106,\n",
       " 'carbohydrates': 107,\n",
       " 'proteins': 108,\n",
       " 'fats': 109,\n",
       " 'improve': 110,\n",
       " 'functions': 111,\n",
       " 'memory': 112,\n",
       " 'concentration': 113,\n",
       " 'particularly': 114,\n",
       " 'beneficial': 115,\n",
       " 'students': 116,\n",
       " 'working': 117,\n",
       " 'professionals': 118,\n",
       " 'enhances': 119,\n",
       " 'allowing': 120,\n",
       " 'them': 121,\n",
       " 'retain': 122,\n",
       " 'information': 123,\n",
       " 'effectively': 124,\n",
       " 'stay': 125,\n",
       " 'focused': 126,\n",
       " 'skipping': 127,\n",
       " 'may': 128,\n",
       " 'lead': 129,\n",
       " 'sluggishness': 130,\n",
       " 'irritability': 131,\n",
       " 'blood': 132,\n",
       " 'remain': 133,\n",
       " 'low': 134,\n",
       " 'impairing': 135,\n",
       " 'diminishing': 136,\n",
       " 'one’s': 137,\n",
       " 'handle': 138,\n",
       " 'tasks': 139,\n",
       " 'efficiently': 140,\n",
       " 'those': 141,\n",
       " 'plays': 142,\n",
       " 'role': 143,\n",
       " 'controlling': 144,\n",
       " 'appetite': 145,\n",
       " 'skip': 146,\n",
       " 'they': 147,\n",
       " 'overcompensate': 148,\n",
       " 'larger': 149,\n",
       " 'portions': 150,\n",
       " 'at': 151,\n",
       " 'subsequent': 152,\n",
       " 'meals': 153,\n",
       " 'snacking': 154,\n",
       " 'calorie': 155,\n",
       " 'curb': 156,\n",
       " 'unhealthy': 157,\n",
       " 'gain': 158,\n",
       " 'including': 159,\n",
       " 'protein': 160,\n",
       " 'rich': 161,\n",
       " 'greek': 162,\n",
       " 'yogurt': 163,\n",
       " 'nuts': 164,\n",
       " 'alongside': 165,\n",
       " 'grains': 166,\n",
       " 'not': 167,\n",
       " 'only': 168,\n",
       " 'curbs': 169,\n",
       " 'but': 170,\n",
       " 'stabilizes': 171,\n",
       " 'ensuring': 172,\n",
       " 'slower': 173,\n",
       " 'release': 174,\n",
       " 'into': 175,\n",
       " 'bloodstream': 176,\n",
       " 'moreover': 177,\n",
       " 'who': 178,\n",
       " 'regularly': 179,\n",
       " 'tend': 180,\n",
       " 'healthier': 181,\n",
       " 'food': 182,\n",
       " 'choices': 183,\n",
       " 'help': 184,\n",
       " 'regulate': 185,\n",
       " 'cravings': 186,\n",
       " 'reduce': 187,\n",
       " 'impulsive': 188,\n",
       " 'another': 189,\n",
       " 'significant': 190,\n",
       " 'benefit': 191,\n",
       " 'its': 192,\n",
       " 'impact': 193,\n",
       " 'metabolic': 194,\n",
       " 'rate': 195,\n",
       " 'we': 196,\n",
       " 'metabolism': 197,\n",
       " 'kicks': 198,\n",
       " 'aiding': 199,\n",
       " 'better': 200,\n",
       " 'digestion': 201,\n",
       " 'nutrient': 202,\n",
       " 'absorption': 203,\n",
       " 'fiber': 204,\n",
       " 'oatmeal': 205,\n",
       " 'supports': 206,\n",
       " 'digestive': 207,\n",
       " 'regular': 208,\n",
       " 'bowel': 209,\n",
       " 'movements': 210,\n",
       " 'preventing': 211,\n",
       " 'constipation': 212,\n",
       " 'vital': 213,\n",
       " 'gut': 214,\n",
       " 'furthermore': 215,\n",
       " 'greatly': 216,\n",
       " 'accommodate': 217,\n",
       " 'dietary': 218,\n",
       " 'restrictions': 219,\n",
       " 'cultural': 220,\n",
       " 'differences': 221,\n",
       " 'personal': 222,\n",
       " 'preferences': 223,\n",
       " 'example': 224,\n",
       " 'traditional': 225,\n",
       " 'western': 226,\n",
       " 'might': 227,\n",
       " 'cereals': 228,\n",
       " 'while': 229,\n",
       " 'asian': 230,\n",
       " 'cultures': 231,\n",
       " 'could': 232,\n",
       " 'consist': 233,\n",
       " 'rice': 234,\n",
       " 'dishes': 235,\n",
       " 'soups': 236,\n",
       " 'dumplings': 237,\n",
       " 'although': 238,\n",
       " 'core': 239,\n",
       " 'goal': 240,\n",
       " 'same': 241,\n",
       " 'provide': 242,\n",
       " 'body': 243,\n",
       " 'nutrients': 244,\n",
       " 'begin': 245,\n",
       " 'additionally': 246,\n",
       " 'opportunity': 247,\n",
       " 'minerals': 248,\n",
       " 'likely': 249,\n",
       " 'consume': 250,\n",
       " 'vegetables': 251,\n",
       " 'dairy': 252,\n",
       " 'meeting': 253,\n",
       " 'recommended': 254,\n",
       " 'intake': 255,\n",
       " 'calcium': 256,\n",
       " 'iron': 257,\n",
       " 'b': 258,\n",
       " 'c': 259,\n",
       " 'maintaining': 260,\n",
       " 'bone': 261,\n",
       " 'muscle': 262,\n",
       " 'function': 263,\n",
       " 'immune': 264,\n",
       " 'response': 265,\n",
       " 'there': 266,\n",
       " 'social': 267,\n",
       " 'aspect': 268,\n",
       " 'sharing': 269,\n",
       " 'family': 270,\n",
       " 'friends': 271,\n",
       " 'foster': 272,\n",
       " 'sense': 273,\n",
       " 'connection': 274,\n",
       " 'routine': 275,\n",
       " 'allows': 276,\n",
       " 'individuals': 277,\n",
       " 'start': 278,\n",
       " 'note': 279,\n",
       " 'encouraging': 280,\n",
       " 'mindful': 281,\n",
       " 'making': 282,\n",
       " 'moment': 283,\n",
       " 'pause': 284,\n",
       " 'before': 285,\n",
       " 'demands': 286,\n",
       " 'take': 287,\n",
       " 'over': 288,\n",
       " 'many': 289,\n",
       " 'busy': 290,\n",
       " 'lifestyles': 291,\n",
       " 'challenging': 292,\n",
       " 'fit': 293,\n",
       " 'proper': 294,\n",
       " 'despite': 295,\n",
       " 'time': 296,\n",
       " 'constraints': 297,\n",
       " 'made': 298,\n",
       " 'accessible': 299,\n",
       " 'quick': 300,\n",
       " 'yet': 301,\n",
       " 'options': 302,\n",
       " 'smoothies': 303,\n",
       " 'muffins': 304,\n",
       " 'bars': 305,\n",
       " 'ultimately': 306,\n",
       " 'powerful': 307,\n",
       " 'tool': 308,\n",
       " 'enhancing': 309,\n",
       " 'clarity': 310,\n",
       " 'whether': 311,\n",
       " 'enjoyed': 312,\n",
       " 'leisurely': 313,\n",
       " 'go': 314,\n",
       " 'contributes': 315,\n",
       " 'substantially': 316,\n",
       " \"one's\": 317,\n",
       " 'underscoring': 318,\n",
       " 'why': 319,\n",
       " 'fundamental': 320,\n",
       " 'part': 321,\n",
       " 'lifestyle': 322}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20, 79, 80, 81, 4, 82, 6, 83, 1, 84, 3, 85, 7, 20, 21, 22, 86, 1, 87, 23, 3, 15]\n",
      "[4, 39, 2, 8, 88, 89, 14, 90, 91, 40, 24, 41, 25, 42, 1, 43, 44, 45]\n",
      "[92, 2, 93, 94, 22, 95, 4, 96, 21, 97, 26, 7, 98, 9, 15, 99, 1, 100, 101]\n",
      "[102, 103, 104, 4, 46, 2, 105, 106, 107, 108, 1, 47, 109, 8, 110, 25, 111, 48, 112, 1, 113]\n",
      "[27, 12, 114, 115, 7, 116, 1, 117, 118, 11, 28, 119, 29, 42, 120, 121, 5, 122, 123, 30, 124, 1, 125, 126]\n",
      "[127, 2, 49, 128, 129, 5, 130, 1, 131, 11, 132, 50, 22, 133, 134, 135, 25, 51, 1, 136, 137, 51, 5, 138, 139, 140]\n",
      "[7, 141, 52, 31, 2, 142, 53, 19, 143, 9, 144, 145, 23, 3, 13]\n",
      "[54, 16, 146, 2, 147, 18, 148, 55, 32, 149, 150, 151, 152, 153, 10, 154, 17, 56, 155, 33, 5, 156, 57, 34, 8, 58, 5, 157, 31, 158]\n",
      "[159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165, 35, 166, 1, 60, 167, 168, 169, 57, 170, 61, 171, 21, 172, 4, 173, 174, 6, 50, 175, 3, 176]\n",
      "[177, 16, 178, 179, 62, 2, 180, 5, 63, 181, 182, 183, 23, 3, 13, 11, 2, 8, 184, 185, 186, 1, 187, 188, 32]\n",
      "[189, 190, 191, 6, 2, 12, 192, 64, 193, 17, 194, 195]\n",
      "[54, 196, 62, 9, 3, 15, 20, 197, 198, 9, 199, 9, 200, 201, 1, 202, 203]\n",
      "[4, 56, 204, 2, 36, 11, 205, 10, 35, 65, 66, 206, 207, 14, 55, 67, 208, 209, 210, 1, 211, 212, 34, 12, 213, 7, 43, 214, 14]\n",
      "[215, 2, 33, 8, 68, 216, 5, 217, 218, 219, 220, 221, 1, 222, 223]\n",
      "[7, 224, 4, 225, 226, 2, 227, 69, 228, 59, 10, 66, 229, 9, 230, 231, 2, 232, 233, 6, 234, 235, 236, 10, 237]\n",
      "[238, 2, 70, 68, 3, 239, 240, 71, 3, 241, 5, 242, 3, 243, 37, 26, 244, 5, 245, 3, 13]\n",
      "[246, 2, 8, 72, 53, 247, 5, 69, 19, 73, 1, 248, 11, 16, 74, 30, 249, 5, 250, 60, 251, 1, 252, 9, 3, 15]\n",
      "[27, 8, 58, 5, 253, 3, 254, 44, 255, 6, 256, 257, 1, 73, 258, 1, 259, 34, 74, 26, 7, 260, 261, 14, 262, 263, 1, 264, 265]\n",
      "[266, 12, 61, 4, 267, 268, 5, 2, 269, 4, 15, 38, 37, 270, 10, 271, 8, 272, 4, 273, 6, 274, 1, 24, 41]\n",
      "[27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64, 279, 18, 280, 281, 32, 70, 1, 282, 2, 4, 283, 5, 284, 285, 3, 286, 6, 3, 13, 287, 288]\n",
      "[7, 289, 16, 49, 290, 291, 63, 28, 292, 5, 293, 9, 4, 294, 2]\n",
      "[295, 296, 297, 2, 8, 72, 298, 30, 299, 37, 300, 301, 39, 302, 36, 11, 303, 35, 65, 304, 10, 2, 305]\n",
      "[306, 2, 12, 4, 307, 308, 7, 309, 14, 52, 31, 1, 67, 29, 310, 1, 45]\n",
      "[311, 312, 313, 10, 17, 3, 314, 4, 24, 46, 2, 315, 316, 5, 317, 40, 1, 29, 14, 318, 319, 28, 71, 36, 4, 320, 321, 6, 4, 47, 322]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "input_sequences=[]\n",
    "for sentence in faqs.split('.'):\n",
    "    tokenized_sentence=tokenizer.texts_to_sequences([sentence])[0]\n",
    "    print(tokenized_sentence)\n",
    "    for i in range(1,len(tokenized_sentence)):\n",
    "        input_sequences.append(tokenized_sentence[:i+1])\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 12], [2, 12, 18], [2, 12, 18, 75], [2, 12, 18, 75, 11], [2, 12, 18, 75, 11, 3], [2, 12, 18, 75, 11, 3, 76], [2, 12, 18, 75, 11, 3, 76, 19], [2, 12, 18, 75, 11, 3, 76, 19, 38], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20, 79], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20, 79, 80], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20, 79, 80, 81], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20, 79, 80, 81, 4], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20, 79, 80, 81, 4, 82], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20, 79, 80, 81, 4, 82, 6], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20, 79, 80, 81, 4, 82, 6, 83], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20, 79, 80, 81, 4, 82, 6, 83, 1], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20, 79, 80, 81, 4, 82, 6, 83, 1, 84], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20, 79, 80, 81, 4, 82, 6, 83, 1, 84, 3], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20, 79, 80, 81, 4, 82, 6, 83, 1, 84, 3, 85], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20, 79, 80, 81, 4, 82, 6, 83, 1, 84, 3, 85, 7], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20, 79, 80, 81, 4, 82, 6, 83, 1, 84, 3, 85, 7, 20], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20, 79, 80, 81, 4, 82, 6, 83, 1, 84, 3, 85, 7, 20, 21], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20, 79, 80, 81, 4, 82, 6, 83, 1, 84, 3, 85, 7, 20, 21, 22], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20, 79, 80, 81, 4, 82, 6, 83, 1, 84, 3, 85, 7, 20, 21, 22, 86], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20, 79, 80, 81, 4, 82, 6, 83, 1, 84, 3, 85, 7, 20, 21, 22, 86, 1], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20, 79, 80, 81, 4, 82, 6, 83, 1, 84, 3, 85, 7, 20, 21, 22, 86, 1, 87], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20, 79, 80, 81, 4, 82, 6, 83, 1, 84, 3, 85, 7, 20, 21, 22, 86, 1, 87, 23], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20, 79, 80, 81, 4, 82, 6, 83, 1, 84, 3, 85, 7, 20, 21, 22, 86, 1, 87, 23, 3], [2, 12, 18, 75, 11, 3, 76, 19, 38, 6, 3, 13, 77, 3, 78, 20, 79, 80, 81, 4, 82, 6, 83, 1, 84, 3, 85, 7, 20, 21, 22, 86, 1, 87, 23, 3, 15], [4, 39], [4, 39, 2], [4, 39, 2, 8], [4, 39, 2, 8, 88], [4, 39, 2, 8, 88, 89], [4, 39, 2, 8, 88, 89, 14], [4, 39, 2, 8, 88, 89, 14, 90], [4, 39, 2, 8, 88, 89, 14, 90, 91], [4, 39, 2, 8, 88, 89, 14, 90, 91, 40], [4, 39, 2, 8, 88, 89, 14, 90, 91, 40, 24], [4, 39, 2, 8, 88, 89, 14, 90, 91, 40, 24, 41], [4, 39, 2, 8, 88, 89, 14, 90, 91, 40, 24, 41, 25], [4, 39, 2, 8, 88, 89, 14, 90, 91, 40, 24, 41, 25, 42], [4, 39, 2, 8, 88, 89, 14, 90, 91, 40, 24, 41, 25, 42, 1], [4, 39, 2, 8, 88, 89, 14, 90, 91, 40, 24, 41, 25, 42, 1, 43], [4, 39, 2, 8, 88, 89, 14, 90, 91, 40, 24, 41, 25, 42, 1, 43, 44], [4, 39, 2, 8, 88, 89, 14, 90, 91, 40, 24, 41, 25, 42, 1, 43, 44, 45], [92, 2], [92, 2, 93], [92, 2, 93, 94], [92, 2, 93, 94, 22], [92, 2, 93, 94, 22, 95], [92, 2, 93, 94, 22, 95, 4], [92, 2, 93, 94, 22, 95, 4, 96], [92, 2, 93, 94, 22, 95, 4, 96, 21], [92, 2, 93, 94, 22, 95, 4, 96, 21, 97], [92, 2, 93, 94, 22, 95, 4, 96, 21, 97, 26], [92, 2, 93, 94, 22, 95, 4, 96, 21, 97, 26, 7], [92, 2, 93, 94, 22, 95, 4, 96, 21, 97, 26, 7, 98], [92, 2, 93, 94, 22, 95, 4, 96, 21, 97, 26, 7, 98, 9], [92, 2, 93, 94, 22, 95, 4, 96, 21, 97, 26, 7, 98, 9, 15], [92, 2, 93, 94, 22, 95, 4, 96, 21, 97, 26, 7, 98, 9, 15, 99], [92, 2, 93, 94, 22, 95, 4, 96, 21, 97, 26, 7, 98, 9, 15, 99, 1], [92, 2, 93, 94, 22, 95, 4, 96, 21, 97, 26, 7, 98, 9, 15, 99, 1, 100], [92, 2, 93, 94, 22, 95, 4, 96, 21, 97, 26, 7, 98, 9, 15, 99, 1, 100, 101], [102, 103], [102, 103, 104], [102, 103, 104, 4], [102, 103, 104, 4, 46], [102, 103, 104, 4, 46, 2], [102, 103, 104, 4, 46, 2, 105], [102, 103, 104, 4, 46, 2, 105, 106], [102, 103, 104, 4, 46, 2, 105, 106, 107], [102, 103, 104, 4, 46, 2, 105, 106, 107, 108], [102, 103, 104, 4, 46, 2, 105, 106, 107, 108, 1], [102, 103, 104, 4, 46, 2, 105, 106, 107, 108, 1, 47], [102, 103, 104, 4, 46, 2, 105, 106, 107, 108, 1, 47, 109], [102, 103, 104, 4, 46, 2, 105, 106, 107, 108, 1, 47, 109, 8], [102, 103, 104, 4, 46, 2, 105, 106, 107, 108, 1, 47, 109, 8, 110], [102, 103, 104, 4, 46, 2, 105, 106, 107, 108, 1, 47, 109, 8, 110, 25], [102, 103, 104, 4, 46, 2, 105, 106, 107, 108, 1, 47, 109, 8, 110, 25, 111], [102, 103, 104, 4, 46, 2, 105, 106, 107, 108, 1, 47, 109, 8, 110, 25, 111, 48], [102, 103, 104, 4, 46, 2, 105, 106, 107, 108, 1, 47, 109, 8, 110, 25, 111, 48, 112], [102, 103, 104, 4, 46, 2, 105, 106, 107, 108, 1, 47, 109, 8, 110, 25, 111, 48, 112, 1], [102, 103, 104, 4, 46, 2, 105, 106, 107, 108, 1, 47, 109, 8, 110, 25, 111, 48, 112, 1, 113], [27, 12], [27, 12, 114], [27, 12, 114, 115], [27, 12, 114, 115, 7], [27, 12, 114, 115, 7, 116], [27, 12, 114, 115, 7, 116, 1], [27, 12, 114, 115, 7, 116, 1, 117], [27, 12, 114, 115, 7, 116, 1, 117, 118], [27, 12, 114, 115, 7, 116, 1, 117, 118, 11], [27, 12, 114, 115, 7, 116, 1, 117, 118, 11, 28], [27, 12, 114, 115, 7, 116, 1, 117, 118, 11, 28, 119], [27, 12, 114, 115, 7, 116, 1, 117, 118, 11, 28, 119, 29], [27, 12, 114, 115, 7, 116, 1, 117, 118, 11, 28, 119, 29, 42], [27, 12, 114, 115, 7, 116, 1, 117, 118, 11, 28, 119, 29, 42, 120], [27, 12, 114, 115, 7, 116, 1, 117, 118, 11, 28, 119, 29, 42, 120, 121], [27, 12, 114, 115, 7, 116, 1, 117, 118, 11, 28, 119, 29, 42, 120, 121, 5], [27, 12, 114, 115, 7, 116, 1, 117, 118, 11, 28, 119, 29, 42, 120, 121, 5, 122], [27, 12, 114, 115, 7, 116, 1, 117, 118, 11, 28, 119, 29, 42, 120, 121, 5, 122, 123], [27, 12, 114, 115, 7, 116, 1, 117, 118, 11, 28, 119, 29, 42, 120, 121, 5, 122, 123, 30], [27, 12, 114, 115, 7, 116, 1, 117, 118, 11, 28, 119, 29, 42, 120, 121, 5, 122, 123, 30, 124], [27, 12, 114, 115, 7, 116, 1, 117, 118, 11, 28, 119, 29, 42, 120, 121, 5, 122, 123, 30, 124, 1], [27, 12, 114, 115, 7, 116, 1, 117, 118, 11, 28, 119, 29, 42, 120, 121, 5, 122, 123, 30, 124, 1, 125], [27, 12, 114, 115, 7, 116, 1, 117, 118, 11, 28, 119, 29, 42, 120, 121, 5, 122, 123, 30, 124, 1, 125, 126], [127, 2], [127, 2, 49], [127, 2, 49, 128], [127, 2, 49, 128, 129], [127, 2, 49, 128, 129, 5], [127, 2, 49, 128, 129, 5, 130], [127, 2, 49, 128, 129, 5, 130, 1], [127, 2, 49, 128, 129, 5, 130, 1, 131], [127, 2, 49, 128, 129, 5, 130, 1, 131, 11], [127, 2, 49, 128, 129, 5, 130, 1, 131, 11, 132], [127, 2, 49, 128, 129, 5, 130, 1, 131, 11, 132, 50], [127, 2, 49, 128, 129, 5, 130, 1, 131, 11, 132, 50, 22], [127, 2, 49, 128, 129, 5, 130, 1, 131, 11, 132, 50, 22, 133], [127, 2, 49, 128, 129, 5, 130, 1, 131, 11, 132, 50, 22, 133, 134], [127, 2, 49, 128, 129, 5, 130, 1, 131, 11, 132, 50, 22, 133, 134, 135], [127, 2, 49, 128, 129, 5, 130, 1, 131, 11, 132, 50, 22, 133, 134, 135, 25], [127, 2, 49, 128, 129, 5, 130, 1, 131, 11, 132, 50, 22, 133, 134, 135, 25, 51], [127, 2, 49, 128, 129, 5, 130, 1, 131, 11, 132, 50, 22, 133, 134, 135, 25, 51, 1], [127, 2, 49, 128, 129, 5, 130, 1, 131, 11, 132, 50, 22, 133, 134, 135, 25, 51, 1, 136], [127, 2, 49, 128, 129, 5, 130, 1, 131, 11, 132, 50, 22, 133, 134, 135, 25, 51, 1, 136, 137], [127, 2, 49, 128, 129, 5, 130, 1, 131, 11, 132, 50, 22, 133, 134, 135, 25, 51, 1, 136, 137, 51], [127, 2, 49, 128, 129, 5, 130, 1, 131, 11, 132, 50, 22, 133, 134, 135, 25, 51, 1, 136, 137, 51, 5], [127, 2, 49, 128, 129, 5, 130, 1, 131, 11, 132, 50, 22, 133, 134, 135, 25, 51, 1, 136, 137, 51, 5, 138], [127, 2, 49, 128, 129, 5, 130, 1, 131, 11, 132, 50, 22, 133, 134, 135, 25, 51, 1, 136, 137, 51, 5, 138, 139], [127, 2, 49, 128, 129, 5, 130, 1, 131, 11, 132, 50, 22, 133, 134, 135, 25, 51, 1, 136, 137, 51, 5, 138, 139, 140], [7, 141], [7, 141, 52], [7, 141, 52, 31], [7, 141, 52, 31, 2], [7, 141, 52, 31, 2, 142], [7, 141, 52, 31, 2, 142, 53], [7, 141, 52, 31, 2, 142, 53, 19], [7, 141, 52, 31, 2, 142, 53, 19, 143], [7, 141, 52, 31, 2, 142, 53, 19, 143, 9], [7, 141, 52, 31, 2, 142, 53, 19, 143, 9, 144], [7, 141, 52, 31, 2, 142, 53, 19, 143, 9, 144, 145], [7, 141, 52, 31, 2, 142, 53, 19, 143, 9, 144, 145, 23], [7, 141, 52, 31, 2, 142, 53, 19, 143, 9, 144, 145, 23, 3], [7, 141, 52, 31, 2, 142, 53, 19, 143, 9, 144, 145, 23, 3, 13], [54, 16], [54, 16, 146], [54, 16, 146, 2], [54, 16, 146, 2, 147], [54, 16, 146, 2, 147, 18], [54, 16, 146, 2, 147, 18, 148], [54, 16, 146, 2, 147, 18, 148, 55], [54, 16, 146, 2, 147, 18, 148, 55, 32], [54, 16, 146, 2, 147, 18, 148, 55, 32, 149], [54, 16, 146, 2, 147, 18, 148, 55, 32, 149, 150], [54, 16, 146, 2, 147, 18, 148, 55, 32, 149, 150, 151], [54, 16, 146, 2, 147, 18, 148, 55, 32, 149, 150, 151, 152], [54, 16, 146, 2, 147, 18, 148, 55, 32, 149, 150, 151, 152, 153], [54, 16, 146, 2, 147, 18, 148, 55, 32, 149, 150, 151, 152, 153, 10], [54, 16, 146, 2, 147, 18, 148, 55, 32, 149, 150, 151, 152, 153, 10, 154], [54, 16, 146, 2, 147, 18, 148, 55, 32, 149, 150, 151, 152, 153, 10, 154, 17], [54, 16, 146, 2, 147, 18, 148, 55, 32, 149, 150, 151, 152, 153, 10, 154, 17, 56], [54, 16, 146, 2, 147, 18, 148, 55, 32, 149, 150, 151, 152, 153, 10, 154, 17, 56, 155], [54, 16, 146, 2, 147, 18, 148, 55, 32, 149, 150, 151, 152, 153, 10, 154, 17, 56, 155, 33], [54, 16, 146, 2, 147, 18, 148, 55, 32, 149, 150, 151, 152, 153, 10, 154, 17, 56, 155, 33, 5], [54, 16, 146, 2, 147, 18, 148, 55, 32, 149, 150, 151, 152, 153, 10, 154, 17, 56, 155, 33, 5, 156], [54, 16, 146, 2, 147, 18, 148, 55, 32, 149, 150, 151, 152, 153, 10, 154, 17, 56, 155, 33, 5, 156, 57], [54, 16, 146, 2, 147, 18, 148, 55, 32, 149, 150, 151, 152, 153, 10, 154, 17, 56, 155, 33, 5, 156, 57, 34], [54, 16, 146, 2, 147, 18, 148, 55, 32, 149, 150, 151, 152, 153, 10, 154, 17, 56, 155, 33, 5, 156, 57, 34, 8], [54, 16, 146, 2, 147, 18, 148, 55, 32, 149, 150, 151, 152, 153, 10, 154, 17, 56, 155, 33, 5, 156, 57, 34, 8, 58], [54, 16, 146, 2, 147, 18, 148, 55, 32, 149, 150, 151, 152, 153, 10, 154, 17, 56, 155, 33, 5, 156, 57, 34, 8, 58, 5], [54, 16, 146, 2, 147, 18, 148, 55, 32, 149, 150, 151, 152, 153, 10, 154, 17, 56, 155, 33, 5, 156, 57, 34, 8, 58, 5, 157], [54, 16, 146, 2, 147, 18, 148, 55, 32, 149, 150, 151, 152, 153, 10, 154, 17, 56, 155, 33, 5, 156, 57, 34, 8, 58, 5, 157, 31], [54, 16, 146, 2, 147, 18, 148, 55, 32, 149, 150, 151, 152, 153, 10, 154, 17, 56, 155, 33, 5, 156, 57, 34, 8, 58, 5, 157, 31, 158], [159, 160], [159, 160, 161], [159, 160, 161, 33], [159, 160, 161, 33, 48], [159, 160, 161, 33, 48, 59], [159, 160, 161, 33, 48, 59, 162], [159, 160, 161, 33, 48, 59, 162, 163], [159, 160, 161, 33, 48, 59, 162, 163, 10], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165, 35], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165, 35, 166], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165, 35, 166, 1], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165, 35, 166, 1, 60], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165, 35, 166, 1, 60, 167], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165, 35, 166, 1, 60, 167, 168], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165, 35, 166, 1, 60, 167, 168, 169], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165, 35, 166, 1, 60, 167, 168, 169, 57], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165, 35, 166, 1, 60, 167, 168, 169, 57, 170], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165, 35, 166, 1, 60, 167, 168, 169, 57, 170, 61], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165, 35, 166, 1, 60, 167, 168, 169, 57, 170, 61, 171], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165, 35, 166, 1, 60, 167, 168, 169, 57, 170, 61, 171, 21], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165, 35, 166, 1, 60, 167, 168, 169, 57, 170, 61, 171, 21, 172], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165, 35, 166, 1, 60, 167, 168, 169, 57, 170, 61, 171, 21, 172, 4], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165, 35, 166, 1, 60, 167, 168, 169, 57, 170, 61, 171, 21, 172, 4, 173], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165, 35, 166, 1, 60, 167, 168, 169, 57, 170, 61, 171, 21, 172, 4, 173, 174], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165, 35, 166, 1, 60, 167, 168, 169, 57, 170, 61, 171, 21, 172, 4, 173, 174, 6], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165, 35, 166, 1, 60, 167, 168, 169, 57, 170, 61, 171, 21, 172, 4, 173, 174, 6, 50], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165, 35, 166, 1, 60, 167, 168, 169, 57, 170, 61, 171, 21, 172, 4, 173, 174, 6, 50, 175], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165, 35, 166, 1, 60, 167, 168, 169, 57, 170, 61, 171, 21, 172, 4, 173, 174, 6, 50, 175, 3], [159, 160, 161, 33, 48, 59, 162, 163, 10, 164, 165, 35, 166, 1, 60, 167, 168, 169, 57, 170, 61, 171, 21, 172, 4, 173, 174, 6, 50, 175, 3, 176], [177, 16], [177, 16, 178], [177, 16, 178, 179], [177, 16, 178, 179, 62], [177, 16, 178, 179, 62, 2], [177, 16, 178, 179, 62, 2, 180], [177, 16, 178, 179, 62, 2, 180, 5], [177, 16, 178, 179, 62, 2, 180, 5, 63], [177, 16, 178, 179, 62, 2, 180, 5, 63, 181], [177, 16, 178, 179, 62, 2, 180, 5, 63, 181, 182], [177, 16, 178, 179, 62, 2, 180, 5, 63, 181, 182, 183], [177, 16, 178, 179, 62, 2, 180, 5, 63, 181, 182, 183, 23], [177, 16, 178, 179, 62, 2, 180, 5, 63, 181, 182, 183, 23, 3], [177, 16, 178, 179, 62, 2, 180, 5, 63, 181, 182, 183, 23, 3, 13], [177, 16, 178, 179, 62, 2, 180, 5, 63, 181, 182, 183, 23, 3, 13, 11], [177, 16, 178, 179, 62, 2, 180, 5, 63, 181, 182, 183, 23, 3, 13, 11, 2], [177, 16, 178, 179, 62, 2, 180, 5, 63, 181, 182, 183, 23, 3, 13, 11, 2, 8], [177, 16, 178, 179, 62, 2, 180, 5, 63, 181, 182, 183, 23, 3, 13, 11, 2, 8, 184], [177, 16, 178, 179, 62, 2, 180, 5, 63, 181, 182, 183, 23, 3, 13, 11, 2, 8, 184, 185], [177, 16, 178, 179, 62, 2, 180, 5, 63, 181, 182, 183, 23, 3, 13, 11, 2, 8, 184, 185, 186], [177, 16, 178, 179, 62, 2, 180, 5, 63, 181, 182, 183, 23, 3, 13, 11, 2, 8, 184, 185, 186, 1], [177, 16, 178, 179, 62, 2, 180, 5, 63, 181, 182, 183, 23, 3, 13, 11, 2, 8, 184, 185, 186, 1, 187], [177, 16, 178, 179, 62, 2, 180, 5, 63, 181, 182, 183, 23, 3, 13, 11, 2, 8, 184, 185, 186, 1, 187, 188], [177, 16, 178, 179, 62, 2, 180, 5, 63, 181, 182, 183, 23, 3, 13, 11, 2, 8, 184, 185, 186, 1, 187, 188, 32], [189, 190], [189, 190, 191], [189, 190, 191, 6], [189, 190, 191, 6, 2], [189, 190, 191, 6, 2, 12], [189, 190, 191, 6, 2, 12, 192], [189, 190, 191, 6, 2, 12, 192, 64], [189, 190, 191, 6, 2, 12, 192, 64, 193], [189, 190, 191, 6, 2, 12, 192, 64, 193, 17], [189, 190, 191, 6, 2, 12, 192, 64, 193, 17, 194], [189, 190, 191, 6, 2, 12, 192, 64, 193, 17, 194, 195], [54, 196], [54, 196, 62], [54, 196, 62, 9], [54, 196, 62, 9, 3], [54, 196, 62, 9, 3, 15], [54, 196, 62, 9, 3, 15, 20], [54, 196, 62, 9, 3, 15, 20, 197], [54, 196, 62, 9, 3, 15, 20, 197, 198], [54, 196, 62, 9, 3, 15, 20, 197, 198, 9], [54, 196, 62, 9, 3, 15, 20, 197, 198, 9, 199], [54, 196, 62, 9, 3, 15, 20, 197, 198, 9, 199, 9], [54, 196, 62, 9, 3, 15, 20, 197, 198, 9, 199, 9, 200], [54, 196, 62, 9, 3, 15, 20, 197, 198, 9, 199, 9, 200, 201], [54, 196, 62, 9, 3, 15, 20, 197, 198, 9, 199, 9, 200, 201, 1], [54, 196, 62, 9, 3, 15, 20, 197, 198, 9, 199, 9, 200, 201, 1, 202], [54, 196, 62, 9, 3, 15, 20, 197, 198, 9, 199, 9, 200, 201, 1, 202, 203], [4, 56], [4, 56, 204], [4, 56, 204, 2], [4, 56, 204, 2, 36], [4, 56, 204, 2, 36, 11], [4, 56, 204, 2, 36, 11, 205], [4, 56, 204, 2, 36, 11, 205, 10], [4, 56, 204, 2, 36, 11, 205, 10, 35], [4, 56, 204, 2, 36, 11, 205, 10, 35, 65], [4, 56, 204, 2, 36, 11, 205, 10, 35, 65, 66], [4, 56, 204, 2, 36, 11, 205, 10, 35, 65, 66, 206], [4, 56, 204, 2, 36, 11, 205, 10, 35, 65, 66, 206, 207], [4, 56, 204, 2, 36, 11, 205, 10, 35, 65, 66, 206, 207, 14], [4, 56, 204, 2, 36, 11, 205, 10, 35, 65, 66, 206, 207, 14, 55], [4, 56, 204, 2, 36, 11, 205, 10, 35, 65, 66, 206, 207, 14, 55, 67], [4, 56, 204, 2, 36, 11, 205, 10, 35, 65, 66, 206, 207, 14, 55, 67, 208], [4, 56, 204, 2, 36, 11, 205, 10, 35, 65, 66, 206, 207, 14, 55, 67, 208, 209], [4, 56, 204, 2, 36, 11, 205, 10, 35, 65, 66, 206, 207, 14, 55, 67, 208, 209, 210], [4, 56, 204, 2, 36, 11, 205, 10, 35, 65, 66, 206, 207, 14, 55, 67, 208, 209, 210, 1], [4, 56, 204, 2, 36, 11, 205, 10, 35, 65, 66, 206, 207, 14, 55, 67, 208, 209, 210, 1, 211], [4, 56, 204, 2, 36, 11, 205, 10, 35, 65, 66, 206, 207, 14, 55, 67, 208, 209, 210, 1, 211, 212], [4, 56, 204, 2, 36, 11, 205, 10, 35, 65, 66, 206, 207, 14, 55, 67, 208, 209, 210, 1, 211, 212, 34], [4, 56, 204, 2, 36, 11, 205, 10, 35, 65, 66, 206, 207, 14, 55, 67, 208, 209, 210, 1, 211, 212, 34, 12], [4, 56, 204, 2, 36, 11, 205, 10, 35, 65, 66, 206, 207, 14, 55, 67, 208, 209, 210, 1, 211, 212, 34, 12, 213], [4, 56, 204, 2, 36, 11, 205, 10, 35, 65, 66, 206, 207, 14, 55, 67, 208, 209, 210, 1, 211, 212, 34, 12, 213, 7], [4, 56, 204, 2, 36, 11, 205, 10, 35, 65, 66, 206, 207, 14, 55, 67, 208, 209, 210, 1, 211, 212, 34, 12, 213, 7, 43], [4, 56, 204, 2, 36, 11, 205, 10, 35, 65, 66, 206, 207, 14, 55, 67, 208, 209, 210, 1, 211, 212, 34, 12, 213, 7, 43, 214], [4, 56, 204, 2, 36, 11, 205, 10, 35, 65, 66, 206, 207, 14, 55, 67, 208, 209, 210, 1, 211, 212, 34, 12, 213, 7, 43, 214, 14], [215, 2], [215, 2, 33], [215, 2, 33, 8], [215, 2, 33, 8, 68], [215, 2, 33, 8, 68, 216], [215, 2, 33, 8, 68, 216, 5], [215, 2, 33, 8, 68, 216, 5, 217], [215, 2, 33, 8, 68, 216, 5, 217, 218], [215, 2, 33, 8, 68, 216, 5, 217, 218, 219], [215, 2, 33, 8, 68, 216, 5, 217, 218, 219, 220], [215, 2, 33, 8, 68, 216, 5, 217, 218, 219, 220, 221], [215, 2, 33, 8, 68, 216, 5, 217, 218, 219, 220, 221, 1], [215, 2, 33, 8, 68, 216, 5, 217, 218, 219, 220, 221, 1, 222], [215, 2, 33, 8, 68, 216, 5, 217, 218, 219, 220, 221, 1, 222, 223], [7, 224], [7, 224, 4], [7, 224, 4, 225], [7, 224, 4, 225, 226], [7, 224, 4, 225, 226, 2], [7, 224, 4, 225, 226, 2, 227], [7, 224, 4, 225, 226, 2, 227, 69], [7, 224, 4, 225, 226, 2, 227, 69, 228], [7, 224, 4, 225, 226, 2, 227, 69, 228, 59], [7, 224, 4, 225, 226, 2, 227, 69, 228, 59, 10], [7, 224, 4, 225, 226, 2, 227, 69, 228, 59, 10, 66], [7, 224, 4, 225, 226, 2, 227, 69, 228, 59, 10, 66, 229], [7, 224, 4, 225, 226, 2, 227, 69, 228, 59, 10, 66, 229, 9], [7, 224, 4, 225, 226, 2, 227, 69, 228, 59, 10, 66, 229, 9, 230], [7, 224, 4, 225, 226, 2, 227, 69, 228, 59, 10, 66, 229, 9, 230, 231], [7, 224, 4, 225, 226, 2, 227, 69, 228, 59, 10, 66, 229, 9, 230, 231, 2], [7, 224, 4, 225, 226, 2, 227, 69, 228, 59, 10, 66, 229, 9, 230, 231, 2, 232], [7, 224, 4, 225, 226, 2, 227, 69, 228, 59, 10, 66, 229, 9, 230, 231, 2, 232, 233], [7, 224, 4, 225, 226, 2, 227, 69, 228, 59, 10, 66, 229, 9, 230, 231, 2, 232, 233, 6], [7, 224, 4, 225, 226, 2, 227, 69, 228, 59, 10, 66, 229, 9, 230, 231, 2, 232, 233, 6, 234], [7, 224, 4, 225, 226, 2, 227, 69, 228, 59, 10, 66, 229, 9, 230, 231, 2, 232, 233, 6, 234, 235], [7, 224, 4, 225, 226, 2, 227, 69, 228, 59, 10, 66, 229, 9, 230, 231, 2, 232, 233, 6, 234, 235, 236], [7, 224, 4, 225, 226, 2, 227, 69, 228, 59, 10, 66, 229, 9, 230, 231, 2, 232, 233, 6, 234, 235, 236, 10], [7, 224, 4, 225, 226, 2, 227, 69, 228, 59, 10, 66, 229, 9, 230, 231, 2, 232, 233, 6, 234, 235, 236, 10, 237], [238, 2], [238, 2, 70], [238, 2, 70, 68], [238, 2, 70, 68, 3], [238, 2, 70, 68, 3, 239], [238, 2, 70, 68, 3, 239, 240], [238, 2, 70, 68, 3, 239, 240, 71], [238, 2, 70, 68, 3, 239, 240, 71, 3], [238, 2, 70, 68, 3, 239, 240, 71, 3, 241], [238, 2, 70, 68, 3, 239, 240, 71, 3, 241, 5], [238, 2, 70, 68, 3, 239, 240, 71, 3, 241, 5, 242], [238, 2, 70, 68, 3, 239, 240, 71, 3, 241, 5, 242, 3], [238, 2, 70, 68, 3, 239, 240, 71, 3, 241, 5, 242, 3, 243], [238, 2, 70, 68, 3, 239, 240, 71, 3, 241, 5, 242, 3, 243, 37], [238, 2, 70, 68, 3, 239, 240, 71, 3, 241, 5, 242, 3, 243, 37, 26], [238, 2, 70, 68, 3, 239, 240, 71, 3, 241, 5, 242, 3, 243, 37, 26, 244], [238, 2, 70, 68, 3, 239, 240, 71, 3, 241, 5, 242, 3, 243, 37, 26, 244, 5], [238, 2, 70, 68, 3, 239, 240, 71, 3, 241, 5, 242, 3, 243, 37, 26, 244, 5, 245], [238, 2, 70, 68, 3, 239, 240, 71, 3, 241, 5, 242, 3, 243, 37, 26, 244, 5, 245, 3], [238, 2, 70, 68, 3, 239, 240, 71, 3, 241, 5, 242, 3, 243, 37, 26, 244, 5, 245, 3, 13], [246, 2], [246, 2, 8], [246, 2, 8, 72], [246, 2, 8, 72, 53], [246, 2, 8, 72, 53, 247], [246, 2, 8, 72, 53, 247, 5], [246, 2, 8, 72, 53, 247, 5, 69], [246, 2, 8, 72, 53, 247, 5, 69, 19], [246, 2, 8, 72, 53, 247, 5, 69, 19, 73], [246, 2, 8, 72, 53, 247, 5, 69, 19, 73, 1], [246, 2, 8, 72, 53, 247, 5, 69, 19, 73, 1, 248], [246, 2, 8, 72, 53, 247, 5, 69, 19, 73, 1, 248, 11], [246, 2, 8, 72, 53, 247, 5, 69, 19, 73, 1, 248, 11, 16], [246, 2, 8, 72, 53, 247, 5, 69, 19, 73, 1, 248, 11, 16, 74], [246, 2, 8, 72, 53, 247, 5, 69, 19, 73, 1, 248, 11, 16, 74, 30], [246, 2, 8, 72, 53, 247, 5, 69, 19, 73, 1, 248, 11, 16, 74, 30, 249], [246, 2, 8, 72, 53, 247, 5, 69, 19, 73, 1, 248, 11, 16, 74, 30, 249, 5], [246, 2, 8, 72, 53, 247, 5, 69, 19, 73, 1, 248, 11, 16, 74, 30, 249, 5, 250], [246, 2, 8, 72, 53, 247, 5, 69, 19, 73, 1, 248, 11, 16, 74, 30, 249, 5, 250, 60], [246, 2, 8, 72, 53, 247, 5, 69, 19, 73, 1, 248, 11, 16, 74, 30, 249, 5, 250, 60, 251], [246, 2, 8, 72, 53, 247, 5, 69, 19, 73, 1, 248, 11, 16, 74, 30, 249, 5, 250, 60, 251, 1], [246, 2, 8, 72, 53, 247, 5, 69, 19, 73, 1, 248, 11, 16, 74, 30, 249, 5, 250, 60, 251, 1, 252], [246, 2, 8, 72, 53, 247, 5, 69, 19, 73, 1, 248, 11, 16, 74, 30, 249, 5, 250, 60, 251, 1, 252, 9], [246, 2, 8, 72, 53, 247, 5, 69, 19, 73, 1, 248, 11, 16, 74, 30, 249, 5, 250, 60, 251, 1, 252, 9, 3], [246, 2, 8, 72, 53, 247, 5, 69, 19, 73, 1, 248, 11, 16, 74, 30, 249, 5, 250, 60, 251, 1, 252, 9, 3, 15], [27, 8], [27, 8, 58], [27, 8, 58, 5], [27, 8, 58, 5, 253], [27, 8, 58, 5, 253, 3], [27, 8, 58, 5, 253, 3, 254], [27, 8, 58, 5, 253, 3, 254, 44], [27, 8, 58, 5, 253, 3, 254, 44, 255], [27, 8, 58, 5, 253, 3, 254, 44, 255, 6], [27, 8, 58, 5, 253, 3, 254, 44, 255, 6, 256], [27, 8, 58, 5, 253, 3, 254, 44, 255, 6, 256, 257], [27, 8, 58, 5, 253, 3, 254, 44, 255, 6, 256, 257, 1], [27, 8, 58, 5, 253, 3, 254, 44, 255, 6, 256, 257, 1, 73], [27, 8, 58, 5, 253, 3, 254, 44, 255, 6, 256, 257, 1, 73, 258], [27, 8, 58, 5, 253, 3, 254, 44, 255, 6, 256, 257, 1, 73, 258, 1], [27, 8, 58, 5, 253, 3, 254, 44, 255, 6, 256, 257, 1, 73, 258, 1, 259], [27, 8, 58, 5, 253, 3, 254, 44, 255, 6, 256, 257, 1, 73, 258, 1, 259, 34], [27, 8, 58, 5, 253, 3, 254, 44, 255, 6, 256, 257, 1, 73, 258, 1, 259, 34, 74], [27, 8, 58, 5, 253, 3, 254, 44, 255, 6, 256, 257, 1, 73, 258, 1, 259, 34, 74, 26], [27, 8, 58, 5, 253, 3, 254, 44, 255, 6, 256, 257, 1, 73, 258, 1, 259, 34, 74, 26, 7], [27, 8, 58, 5, 253, 3, 254, 44, 255, 6, 256, 257, 1, 73, 258, 1, 259, 34, 74, 26, 7, 260], [27, 8, 58, 5, 253, 3, 254, 44, 255, 6, 256, 257, 1, 73, 258, 1, 259, 34, 74, 26, 7, 260, 261], [27, 8, 58, 5, 253, 3, 254, 44, 255, 6, 256, 257, 1, 73, 258, 1, 259, 34, 74, 26, 7, 260, 261, 14], [27, 8, 58, 5, 253, 3, 254, 44, 255, 6, 256, 257, 1, 73, 258, 1, 259, 34, 74, 26, 7, 260, 261, 14, 262], [27, 8, 58, 5, 253, 3, 254, 44, 255, 6, 256, 257, 1, 73, 258, 1, 259, 34, 74, 26, 7, 260, 261, 14, 262, 263], [27, 8, 58, 5, 253, 3, 254, 44, 255, 6, 256, 257, 1, 73, 258, 1, 259, 34, 74, 26, 7, 260, 261, 14, 262, 263, 1], [27, 8, 58, 5, 253, 3, 254, 44, 255, 6, 256, 257, 1, 73, 258, 1, 259, 34, 74, 26, 7, 260, 261, 14, 262, 263, 1, 264], [27, 8, 58, 5, 253, 3, 254, 44, 255, 6, 256, 257, 1, 73, 258, 1, 259, 34, 74, 26, 7, 260, 261, 14, 262, 263, 1, 264, 265], [266, 12], [266, 12, 61], [266, 12, 61, 4], [266, 12, 61, 4, 267], [266, 12, 61, 4, 267, 268], [266, 12, 61, 4, 267, 268, 5], [266, 12, 61, 4, 267, 268, 5, 2], [266, 12, 61, 4, 267, 268, 5, 2, 269], [266, 12, 61, 4, 267, 268, 5, 2, 269, 4], [266, 12, 61, 4, 267, 268, 5, 2, 269, 4, 15], [266, 12, 61, 4, 267, 268, 5, 2, 269, 4, 15, 38], [266, 12, 61, 4, 267, 268, 5, 2, 269, 4, 15, 38, 37], [266, 12, 61, 4, 267, 268, 5, 2, 269, 4, 15, 38, 37, 270], [266, 12, 61, 4, 267, 268, 5, 2, 269, 4, 15, 38, 37, 270, 10], [266, 12, 61, 4, 267, 268, 5, 2, 269, 4, 15, 38, 37, 270, 10, 271], [266, 12, 61, 4, 267, 268, 5, 2, 269, 4, 15, 38, 37, 270, 10, 271, 8], [266, 12, 61, 4, 267, 268, 5, 2, 269, 4, 15, 38, 37, 270, 10, 271, 8, 272], [266, 12, 61, 4, 267, 268, 5, 2, 269, 4, 15, 38, 37, 270, 10, 271, 8, 272, 4], [266, 12, 61, 4, 267, 268, 5, 2, 269, 4, 15, 38, 37, 270, 10, 271, 8, 272, 4, 273], [266, 12, 61, 4, 267, 268, 5, 2, 269, 4, 15, 38, 37, 270, 10, 271, 8, 272, 4, 273, 6], [266, 12, 61, 4, 267, 268, 5, 2, 269, 4, 15, 38, 37, 270, 10, 271, 8, 272, 4, 273, 6, 274], [266, 12, 61, 4, 267, 268, 5, 2, 269, 4, 15, 38, 37, 270, 10, 271, 8, 272, 4, 273, 6, 274, 1], [266, 12, 61, 4, 267, 268, 5, 2, 269, 4, 15, 38, 37, 270, 10, 271, 8, 272, 4, 273, 6, 274, 1, 24], [266, 12, 61, 4, 267, 268, 5, 2, 269, 4, 15, 38, 37, 270, 10, 271, 8, 272, 4, 273, 6, 274, 1, 24, 41], [27, 275], [27, 275, 276], [27, 275, 276, 277], [27, 275, 276, 277, 5], [27, 275, 276, 277, 5, 278], [27, 275, 276, 277, 5, 278, 3], [27, 275, 276, 277, 5, 278, 3, 13], [27, 275, 276, 277, 5, 278, 3, 13, 17], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64, 279], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64, 279, 18], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64, 279, 18, 280], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64, 279, 18, 280, 281], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64, 279, 18, 280, 281, 32], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64, 279, 18, 280, 281, 32, 70], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64, 279, 18, 280, 281, 32, 70, 1], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64, 279, 18, 280, 281, 32, 70, 1, 282], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64, 279, 18, 280, 281, 32, 70, 1, 282, 2], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64, 279, 18, 280, 281, 32, 70, 1, 282, 2, 4], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64, 279, 18, 280, 281, 32, 70, 1, 282, 2, 4, 283], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64, 279, 18, 280, 281, 32, 70, 1, 282, 2, 4, 283, 5], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64, 279, 18, 280, 281, 32, 70, 1, 282, 2, 4, 283, 5, 284], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64, 279, 18, 280, 281, 32, 70, 1, 282, 2, 4, 283, 5, 284, 285], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64, 279, 18, 280, 281, 32, 70, 1, 282, 2, 4, 283, 5, 284, 285, 3], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64, 279, 18, 280, 281, 32, 70, 1, 282, 2, 4, 283, 5, 284, 285, 3, 286], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64, 279, 18, 280, 281, 32, 70, 1, 282, 2, 4, 283, 5, 284, 285, 3, 286, 6], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64, 279, 18, 280, 281, 32, 70, 1, 282, 2, 4, 283, 5, 284, 285, 3, 286, 6, 3], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64, 279, 18, 280, 281, 32, 70, 1, 282, 2, 4, 283, 5, 284, 285, 3, 286, 6, 3, 13], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64, 279, 18, 280, 281, 32, 70, 1, 282, 2, 4, 283, 5, 284, 285, 3, 286, 6, 3, 13, 287], [27, 275, 276, 277, 5, 278, 3, 13, 17, 4, 64, 279, 18, 280, 281, 32, 70, 1, 282, 2, 4, 283, 5, 284, 285, 3, 286, 6, 3, 13, 287, 288], [7, 289], [7, 289, 16], [7, 289, 16, 49], [7, 289, 16, 49, 290], [7, 289, 16, 49, 290, 291], [7, 289, 16, 49, 290, 291, 63], [7, 289, 16, 49, 290, 291, 63, 28], [7, 289, 16, 49, 290, 291, 63, 28, 292], [7, 289, 16, 49, 290, 291, 63, 28, 292, 5], [7, 289, 16, 49, 290, 291, 63, 28, 292, 5, 293], [7, 289, 16, 49, 290, 291, 63, 28, 292, 5, 293, 9], [7, 289, 16, 49, 290, 291, 63, 28, 292, 5, 293, 9, 4], [7, 289, 16, 49, 290, 291, 63, 28, 292, 5, 293, 9, 4, 294], [7, 289, 16, 49, 290, 291, 63, 28, 292, 5, 293, 9, 4, 294, 2], [295, 296], [295, 296, 297], [295, 296, 297, 2], [295, 296, 297, 2, 8], [295, 296, 297, 2, 8, 72], [295, 296, 297, 2, 8, 72, 298], [295, 296, 297, 2, 8, 72, 298, 30], [295, 296, 297, 2, 8, 72, 298, 30, 299], [295, 296, 297, 2, 8, 72, 298, 30, 299, 37], [295, 296, 297, 2, 8, 72, 298, 30, 299, 37, 300], [295, 296, 297, 2, 8, 72, 298, 30, 299, 37, 300, 301], [295, 296, 297, 2, 8, 72, 298, 30, 299, 37, 300, 301, 39], [295, 296, 297, 2, 8, 72, 298, 30, 299, 37, 300, 301, 39, 302], [295, 296, 297, 2, 8, 72, 298, 30, 299, 37, 300, 301, 39, 302, 36], [295, 296, 297, 2, 8, 72, 298, 30, 299, 37, 300, 301, 39, 302, 36, 11], [295, 296, 297, 2, 8, 72, 298, 30, 299, 37, 300, 301, 39, 302, 36, 11, 303], [295, 296, 297, 2, 8, 72, 298, 30, 299, 37, 300, 301, 39, 302, 36, 11, 303, 35], [295, 296, 297, 2, 8, 72, 298, 30, 299, 37, 300, 301, 39, 302, 36, 11, 303, 35, 65], [295, 296, 297, 2, 8, 72, 298, 30, 299, 37, 300, 301, 39, 302, 36, 11, 303, 35, 65, 304], [295, 296, 297, 2, 8, 72, 298, 30, 299, 37, 300, 301, 39, 302, 36, 11, 303, 35, 65, 304, 10], [295, 296, 297, 2, 8, 72, 298, 30, 299, 37, 300, 301, 39, 302, 36, 11, 303, 35, 65, 304, 10, 2], [295, 296, 297, 2, 8, 72, 298, 30, 299, 37, 300, 301, 39, 302, 36, 11, 303, 35, 65, 304, 10, 2, 305], [306, 2], [306, 2, 12], [306, 2, 12, 4], [306, 2, 12, 4, 307], [306, 2, 12, 4, 307, 308], [306, 2, 12, 4, 307, 308, 7], [306, 2, 12, 4, 307, 308, 7, 309], [306, 2, 12, 4, 307, 308, 7, 309, 14], [306, 2, 12, 4, 307, 308, 7, 309, 14, 52], [306, 2, 12, 4, 307, 308, 7, 309, 14, 52, 31], [306, 2, 12, 4, 307, 308, 7, 309, 14, 52, 31, 1], [306, 2, 12, 4, 307, 308, 7, 309, 14, 52, 31, 1, 67], [306, 2, 12, 4, 307, 308, 7, 309, 14, 52, 31, 1, 67, 29], [306, 2, 12, 4, 307, 308, 7, 309, 14, 52, 31, 1, 67, 29, 310], [306, 2, 12, 4, 307, 308, 7, 309, 14, 52, 31, 1, 67, 29, 310, 1], [306, 2, 12, 4, 307, 308, 7, 309, 14, 52, 31, 1, 67, 29, 310, 1, 45], [311, 312], [311, 312, 313], [311, 312, 313, 10], [311, 312, 313, 10, 17], [311, 312, 313, 10, 17, 3], [311, 312, 313, 10, 17, 3, 314], [311, 312, 313, 10, 17, 3, 314, 4], [311, 312, 313, 10, 17, 3, 314, 4, 24], [311, 312, 313, 10, 17, 3, 314, 4, 24, 46], [311, 312, 313, 10, 17, 3, 314, 4, 24, 46, 2], [311, 312, 313, 10, 17, 3, 314, 4, 24, 46, 2, 315], [311, 312, 313, 10, 17, 3, 314, 4, 24, 46, 2, 315, 316], [311, 312, 313, 10, 17, 3, 314, 4, 24, 46, 2, 315, 316, 5], [311, 312, 313, 10, 17, 3, 314, 4, 24, 46, 2, 315, 316, 5, 317], [311, 312, 313, 10, 17, 3, 314, 4, 24, 46, 2, 315, 316, 5, 317, 40], [311, 312, 313, 10, 17, 3, 314, 4, 24, 46, 2, 315, 316, 5, 317, 40, 1], [311, 312, 313, 10, 17, 3, 314, 4, 24, 46, 2, 315, 316, 5, 317, 40, 1, 29], [311, 312, 313, 10, 17, 3, 314, 4, 24, 46, 2, 315, 316, 5, 317, 40, 1, 29, 14], [311, 312, 313, 10, 17, 3, 314, 4, 24, 46, 2, 315, 316, 5, 317, 40, 1, 29, 14, 318], [311, 312, 313, 10, 17, 3, 314, 4, 24, 46, 2, 315, 316, 5, 317, 40, 1, 29, 14, 318, 319], [311, 312, 313, 10, 17, 3, 314, 4, 24, 46, 2, 315, 316, 5, 317, 40, 1, 29, 14, 318, 319, 28], [311, 312, 313, 10, 17, 3, 314, 4, 24, 46, 2, 315, 316, 5, 317, 40, 1, 29, 14, 318, 319, 28, 71], [311, 312, 313, 10, 17, 3, 314, 4, 24, 46, 2, 315, 316, 5, 317, 40, 1, 29, 14, 318, 319, 28, 71, 36], [311, 312, 313, 10, 17, 3, 314, 4, 24, 46, 2, 315, 316, 5, 317, 40, 1, 29, 14, 318, 319, 28, 71, 36, 4], [311, 312, 313, 10, 17, 3, 314, 4, 24, 46, 2, 315, 316, 5, 317, 40, 1, 29, 14, 318, 319, 28, 71, 36, 4, 320], [311, 312, 313, 10, 17, 3, 314, 4, 24, 46, 2, 315, 316, 5, 317, 40, 1, 29, 14, 318, 319, 28, 71, 36, 4, 320, 321], [311, 312, 313, 10, 17, 3, 314, 4, 24, 46, 2, 315, 316, 5, 317, 40, 1, 29, 14, 318, 319, 28, 71, 36, 4, 320, 321, 6], [311, 312, 313, 10, 17, 3, 314, 4, 24, 46, 2, 315, 316, 5, 317, 40, 1, 29, 14, 318, 319, 28, 71, 36, 4, 320, 321, 6, 4], [311, 312, 313, 10, 17, 3, 314, 4, 24, 46, 2, 315, 316, 5, 317, 40, 1, 29, 14, 318, 319, 28, 71, 36, 4, 320, 321, 6, 4, 47], [311, 312, 313, 10, 17, 3, 314, 4, 24, 46, 2, 315, 316, 5, 317, 40, 1, 29, 14, 318, 319, 28, 71, 36, 4, 320, 321, 6, 4, 47, 322]]\n"
     ]
    }
   ],
   "source": [
    "print(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=max([len(x) for x in input_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_input_sequences=pad_sequences(input_sequences,maxlen=max_len,padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   0   2  12]\n",
      " [  0   0   0 ...   2  12  18]\n",
      " [  0   0   0 ...  12  18  75]\n",
      " ...\n",
      " [  0   0   0 ... 321   6   4]\n",
      " [  0   0   0 ...   6   4  47]\n",
      " [  0   0   0 ...   4  47 322]]\n"
     ]
    }
   ],
   "source": [
    "print(padded_input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=padded_input_sequences[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   0   0   2]\n",
      " [  0   0   0 ...   0   2  12]\n",
      " [  0   0   0 ...   2  12  18]\n",
      " ...\n",
      " [  0   0   0 ... 320 321   6]\n",
      " [  0   0   0 ... 321   6   4]\n",
      " [  0   0   0 ...   6   4  47]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=padded_input_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 12  18  75  11   3  76  19  38   6   3  13  77   3  78  20  79  80  81\n",
      "   4  82   6  83   1  84   3  85   7  20  21  22  86   1  87  23   3  15\n",
      "  39   2   8  88  89  14  90  91  40  24  41  25  42   1  43  44  45   2\n",
      "  93  94  22  95   4  96  21  97  26   7  98   9  15  99   1 100 101 103\n",
      " 104   4  46   2 105 106 107 108   1  47 109   8 110  25 111  48 112   1\n",
      " 113  12 114 115   7 116   1 117 118  11  28 119  29  42 120 121   5 122\n",
      " 123  30 124   1 125 126   2  49 128 129   5 130   1 131  11 132  50  22\n",
      " 133 134 135  25  51   1 136 137  51   5 138 139 140 141  52  31   2 142\n",
      "  53  19 143   9 144 145  23   3  13  16 146   2 147  18 148  55  32 149\n",
      " 150 151 152 153  10 154  17  56 155  33   5 156  57  34   8  58   5 157\n",
      "  31 158 160 161  33  48  59 162 163  10 164 165  35 166   1  60 167 168\n",
      " 169  57 170  61 171  21 172   4 173 174   6  50 175   3 176  16 178 179\n",
      "  62   2 180   5  63 181 182 183  23   3  13  11   2   8 184 185 186   1\n",
      " 187 188  32 190 191   6   2  12 192  64 193  17 194 195 196  62   9   3\n",
      "  15  20 197 198   9 199   9 200 201   1 202 203  56 204   2  36  11 205\n",
      "  10  35  65  66 206 207  14  55  67 208 209 210   1 211 212  34  12 213\n",
      "   7  43 214  14   2  33   8  68 216   5 217 218 219 220 221   1 222 223\n",
      " 224   4 225 226   2 227  69 228  59  10  66 229   9 230 231   2 232 233\n",
      "   6 234 235 236  10 237   2  70  68   3 239 240  71   3 241   5 242   3\n",
      " 243  37  26 244   5 245   3  13   2   8  72  53 247   5  69  19  73   1\n",
      " 248  11  16  74  30 249   5 250  60 251   1 252   9   3  15   8  58   5\n",
      " 253   3 254  44 255   6 256 257   1  73 258   1 259  34  74  26   7 260\n",
      " 261  14 262 263   1 264 265  12  61   4 267 268   5   2 269   4  15  38\n",
      "  37 270  10 271   8 272   4 273   6 274   1  24  41 275 276 277   5 278\n",
      "   3  13  17   4  64 279  18 280 281  32  70   1 282   2   4 283   5 284\n",
      " 285   3 286   6   3  13 287 288 289  16  49 290 291  63  28 292   5 293\n",
      "   9   4 294   2 296 297   2   8  72 298  30 299  37 300 301  39 302  36\n",
      "  11 303  35  65 304  10   2 305   2  12   4 307 308   7 309  14  52  31\n",
      "   1  67  29 310   1  45 312 313  10  17   3 314   4  24  46   2 315 316\n",
      "   5 317  40   1  29  14 318 319  28  71  36   4 320 321   6   4  47 322]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(540, 36)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(540,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y=to_categorical(y,num_classes=323)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(540, 323)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayush\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ayush\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=323,output_dim=100,input_length=36,input_shape=(36,)))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(323,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,300</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">150,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">323</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">48,773</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_9 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m32,300\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_8 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)            │       \u001b[38;5;34m150,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m323\u001b[0m)            │        \u001b[38;5;34m48,773\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">231,673</span> (904.97 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m231,673\u001b[0m (904.97 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">231,673</span> (904.97 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m231,673\u001b[0m (904.97 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.0365 - loss: 5.7605\n",
      "Epoch 2/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0366 - loss: 5.4731 \n",
      "Epoch 3/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0610 - loss: 5.3300\n",
      "Epoch 4/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0750 - loss: 5.2071\n",
      "Epoch 5/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0670 - loss: 5.2031\n",
      "Epoch 6/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0748 - loss: 5.1500\n",
      "Epoch 7/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0645 - loss: 5.0858\n",
      "Epoch 8/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0690 - loss: 4.9286\n",
      "Epoch 9/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0541 - loss: 4.9141 \n",
      "Epoch 10/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0869 - loss: 4.7571\n",
      "Epoch 11/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0951 - loss: 4.5925\n",
      "Epoch 12/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0906 - loss: 4.4861\n",
      "Epoch 13/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.1059 - loss: 4.3680\n",
      "Epoch 14/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.1132 - loss: 4.2120\n",
      "Epoch 15/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1186 - loss: 4.1044\n",
      "Epoch 16/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1065 - loss: 3.9898\n",
      "Epoch 17/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1496 - loss: 3.8410\n",
      "Epoch 18/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1837 - loss: 3.6333\n",
      "Epoch 19/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1739 - loss: 3.5837\n",
      "Epoch 20/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2478 - loss: 3.2923\n",
      "Epoch 21/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2751 - loss: 3.1774\n",
      "Epoch 22/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3128 - loss: 3.0621\n",
      "Epoch 23/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3324 - loss: 2.9738\n",
      "Epoch 24/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3529 - loss: 2.7999\n",
      "Epoch 25/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4430 - loss: 2.5613\n",
      "Epoch 26/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5326 - loss: 2.3808\n",
      "Epoch 27/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5377 - loss: 2.3336\n",
      "Epoch 28/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5846 - loss: 2.2374\n",
      "Epoch 29/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6268 - loss: 2.0487\n",
      "Epoch 30/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6748 - loss: 1.9585\n",
      "Epoch 31/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7543 - loss: 1.7928\n",
      "Epoch 32/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7413 - loss: 1.8010\n",
      "Epoch 33/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7873 - loss: 1.6656\n",
      "Epoch 34/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8240 - loss: 1.5401\n",
      "Epoch 35/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8592 - loss: 1.4354\n",
      "Epoch 36/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8454 - loss: 1.3878\n",
      "Epoch 37/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8896 - loss: 1.3156\n",
      "Epoch 38/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9157 - loss: 1.1840\n",
      "Epoch 39/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9281 - loss: 1.1127\n",
      "Epoch 40/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9264 - loss: 1.0666\n",
      "Epoch 41/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9431 - loss: 1.0155\n",
      "Epoch 42/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9589 - loss: 0.9336\n",
      "Epoch 43/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9567 - loss: 0.9076\n",
      "Epoch 44/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9610 - loss: 0.8596\n",
      "Epoch 45/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9633 - loss: 0.7989\n",
      "Epoch 46/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9685 - loss: 0.7757\n",
      "Epoch 47/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9768 - loss: 0.7198\n",
      "Epoch 48/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9575 - loss: 0.6811\n",
      "Epoch 49/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9561 - loss: 0.6978\n",
      "Epoch 50/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9716 - loss: 0.6177\n",
      "Epoch 51/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9646 - loss: 0.6049\n",
      "Epoch 52/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9754 - loss: 0.5535\n",
      "Epoch 53/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9732 - loss: 0.5218\n",
      "Epoch 54/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9765 - loss: 0.4881\n",
      "Epoch 55/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9771 - loss: 0.4555\n",
      "Epoch 56/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9771 - loss: 0.4591\n",
      "Epoch 57/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9784 - loss: 0.4417\n",
      "Epoch 58/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9707 - loss: 0.4075\n",
      "Epoch 59/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9895 - loss: 0.3598\n",
      "Epoch 60/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9853 - loss: 0.3641\n",
      "Epoch 61/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9898 - loss: 0.3276\n",
      "Epoch 62/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9792 - loss: 0.3459\n",
      "Epoch 63/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9801 - loss: 0.3114\n",
      "Epoch 64/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9774 - loss: 0.3122\n",
      "Epoch 65/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9795 - loss: 0.2925\n",
      "Epoch 66/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9866 - loss: 0.2785\n",
      "Epoch 67/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9793 - loss: 0.2816\n",
      "Epoch 68/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9909 - loss: 0.2487\n",
      "Epoch 69/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9867 - loss: 0.2538\n",
      "Epoch 70/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9888 - loss: 0.2300\n",
      "Epoch 71/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9776 - loss: 0.2354\n",
      "Epoch 72/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9847 - loss: 0.2222\n",
      "Epoch 73/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9783 - loss: 0.2215\n",
      "Epoch 74/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9871 - loss: 0.1927\n",
      "Epoch 75/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9840 - loss: 0.1978\n",
      "Epoch 76/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9894 - loss: 0.1832\n",
      "Epoch 77/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9948 - loss: 0.1766\n",
      "Epoch 78/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9922 - loss: 0.1655\n",
      "Epoch 79/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9883 - loss: 0.1657\n",
      "Epoch 80/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9882 - loss: 0.1640\n",
      "Epoch 81/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9870 - loss: 0.1674\n",
      "Epoch 82/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9857 - loss: 0.1653\n",
      "Epoch 83/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9865 - loss: 0.1512\n",
      "Epoch 84/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9894 - loss: 0.1521\n",
      "Epoch 85/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9836 - loss: 0.1504\n",
      "Epoch 86/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9844 - loss: 0.1479\n",
      "Epoch 87/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9894 - loss: 0.1281\n",
      "Epoch 88/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9875 - loss: 0.1297\n",
      "Epoch 89/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9793 - loss: 0.1360\n",
      "Epoch 90/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9908 - loss: 0.1266\n",
      "Epoch 91/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9928 - loss: 0.1136\n",
      "Epoch 92/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9885 - loss: 0.1128\n",
      "Epoch 93/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9905 - loss: 0.1099\n",
      "Epoch 94/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9884 - loss: 0.1188\n",
      "Epoch 95/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9839 - loss: 0.1101\n",
      "Epoch 96/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9772 - loss: 0.1116\n",
      "Epoch 97/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9848 - loss: 0.1007\n",
      "Epoch 98/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9830 - loss: 0.0946\n",
      "Epoch 99/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9892 - loss: 0.0987\n",
      "Epoch 100/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9856 - loss: 0.0989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23eeabc9870>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x,y,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "text=\"Breakfast\"  #tokenize,padding,predict\n",
    "token_text=tokenizer.texts_to_sequences([text])[0]\n",
    "padded_token_text=pad_sequences([token_text],maxlen=36,padding='pre')\n",
    "print(padded_token_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 323)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(padded_token_text).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "pos=np.argmax(model.predict(padded_token_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n"
     ]
    }
   ],
   "source": [
    "for word,index in tokenizer.word_index.items():\n",
    "    if index==pos:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Breakfast is often hailed as the most important meal of the day\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Breakfast is often hailed as the most important meal of the day providing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Breakfast is often hailed as the most important meal of the day providing the\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Breakfast is often hailed as the most important meal of the day providing the fuel\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Breakfast is often hailed as the most important meal of the day providing the fuel our\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for i in range(5):\n",
    "    token_text=tokenizer.texts_to_sequences([text])[0]\n",
    "    padded_token_text=pad_sequences([token_text],maxlen=36,padding='pre')\n",
    "    pos=np.argmax(model.predict(padded_token_text))\n",
    "    for word,index in tokenizer.word_index.items():\n",
    "        if index==pos:\n",
    "            text=text+\" \"+word\n",
    "            print(text)\n",
    "            time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
